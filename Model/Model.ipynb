{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81612cd2-05e3-4a5a-981b-8ee6a59f8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "DATA_ROOT = Path(\"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495e22c-78ba-4440-b5b5-2d861b0452b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv(DATA_ROOT/\"meta_train.csv\")\n",
    "pseudo_train_data = pd.read_csv(DATA_ROOT/\"pseudo_train.csv\")\n",
    "X_data, y_data = list(train_data[\"Filename\"]), list(train_data[\"Label\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, shuffle=True, random_state=87)\n",
    "\n",
    "# Pseudo data\n",
    "X_train = [f\"train/{x}\" for x in X_train] + list(pseudo_train_data['Filename'])\n",
    "y_train = y_train + list(pseudo_train_data['Label'])\n",
    "\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_val, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4de3a7-c3e8-434b-af0b-7af4bb876888",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_train, all_y_train = list(), list()\n",
    "for x, y in zip(X_train, y_train):\n",
    "    all_X_train.append(x)\n",
    "    all_y_train.append(int(y))\n",
    "    # speed\n",
    "    all_X_train.append(f\"augmented/1.1/{x}\")\n",
    "    all_y_train.append(int(y))\n",
    "    all_X_train.append(f\"augmented/0.9/{x}\")\n",
    "    all_y_train.append(int(y))\n",
    "    \n",
    "    # noise\n",
    "    for i in range(5):\n",
    "        all_X_train.append(f\"augmented/noise/{x}_{i}\")\n",
    "        all_y_train.append(int(y))\n",
    "    \n",
    "print(len(all_X_train), len(all_y_train))\n",
    "\n",
    "all_X_val = [f\"train/{x}\" for x in X_val]\n",
    "all_y_val = list(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80dca0-6ac1-440c-b1fd-b290b35d3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight(class_weight='balanced', classes=[0,1,2,3,4,5], y=y_train)\n",
    "print('class weight', class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30101bb-7141-4bc3-aeed-dac78167fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = EasyDict({\n",
    "    'clip_length': 5.0,\n",
    "    'sample_rate': 16000,\n",
    "    'hop_length': 160,\n",
    "    'n_fft': 400,\n",
    "    'n_mels': 64,\n",
    "    'f_min': 0,\n",
    "    'f_max': 8000,\n",
    "})\n",
    "cfg.unit_length = int((cfg.clip_length * cfg.sample_rate + cfg.hop_length - 1) // cfg.hop_length)\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c2383-55e8-4a62-abce-b040c4235227",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=cfg.sample_rate, n_fft=cfg.n_fft, n_mels=cfg.n_mels,\n",
    "    hop_length=cfg.hop_length, f_min=cfg.f_min, f_max=cfg.f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a948f73-9d77-41c7-8e32-54f1cb340ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_list = list()\n",
    "# for f in tqdm(X_train):\n",
    "#     try:\n",
    "#         waveform, sr = torchaudio.load(DATA_ROOT/f\"{f}.wav\")\n",
    "#     except:\n",
    "#         print((DATA_ROOT/f\"{f}.wav\").stat())\n",
    "#         print(f)\n",
    "#     if sr != cfg.sample_rate:\n",
    "#         waveform = torchaudio.transforms.Resample(sr, cfg.sample_rate)(waveform)\n",
    "    \n",
    "#     if waveform.shape[-1] < 80000:\n",
    "#         padding = 80000 - waveform.shape[-1]\n",
    "#         waveform = torch.cat([waveform, torch.zeros([1, padding])], axis=-1)\n",
    "#     elif waveform.shape[-1] > 80000:\n",
    "#         waveform = waveform[:80000]\n",
    "    \n",
    "#     log_mel_spec = (to_mel_spectrogram(waveform) + torch.finfo(torch.float).eps).log()\n",
    "#     spec_list.append(log_mel_spec)\n",
    "    \n",
    "# all_train_lms = np.hstack(spec_list)\n",
    "# train_mean_std = all_train_lms.mean(), all_train_lms.std()\n",
    "\n",
    "# ---\n",
    "# Cached\n",
    "train_mean_std = (-5.2729697, 5.651953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff5310-06d3-4f0f-89a9-79b3d20830ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_length(log_mel_spec):\n",
    "    return log_mel_spec.shape[-1]\n",
    "\n",
    "class DogDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cfg, filenames, labels, norm_mean_std=None):\n",
    "        assert len(filenames) == len(labels), f'Inconsistent length of filenames and labels.'\n",
    "\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.norm_mean_std = norm_mean_std\n",
    "\n",
    "        # Calculate length of clip this dataset will make\n",
    "        self.unit_length = cfg.unit_length\n",
    "\n",
    "        # Test with first file\n",
    "        assert self[0][0].shape[-1] == self.unit_length, f'Check your files, failed to load {filenames[0]}'\n",
    "\n",
    "        # Show basic info.\n",
    "        print(f'Dataset will yield log-mel spectrogram {len(self)} data samples in shape [1, {cfg.n_mels}, {self.unit_length}]')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert 0 <= index and index < len(self)\n",
    "        f = self.filenames[index]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(DATA_ROOT/f\"{f}.wav\")\n",
    "        except:\n",
    "            print(f)\n",
    "        if sr != cfg.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, cfg.sample_rate)(waveform)\n",
    "        \n",
    "        mel_spec = to_mel_spectrogram(waveform)\n",
    "        mel_spec = torchaudio.transforms.TimeMasking(time_mask_param=80)(mel_spec)\n",
    "        mel_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)(mel_spec)\n",
    "        \n",
    "        log_mel_spec = (mel_spec + torch.finfo(torch.float).eps).log()\n",
    "        \n",
    "        # normalize - instance based\n",
    "        if self.norm_mean_std is not None:\n",
    "            log_mel_spec = (log_mel_spec - self.norm_mean_std[0]) / self.norm_mean_std[1]\n",
    "\n",
    "        # Padding if sample is shorter than expected - both head & tail are filled with 0s\n",
    "        pad_size = self.unit_length - sample_length(log_mel_spec)\n",
    "        if pad_size > 0:\n",
    "            offset = pad_size // 2\n",
    "            log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, 0), (offset, pad_size - offset)), 'constant')\n",
    "\n",
    "        # Random crop\n",
    "        crop_size = sample_length(log_mel_spec) - self.unit_length\n",
    "        if crop_size > 0:\n",
    "            start = np.random.randint(0, crop_size)\n",
    "            log_mel_spec = log_mel_spec[..., start:start + self.unit_length]\n",
    "\n",
    "        # Apply augmentations\n",
    "        log_mel_spec = torch.Tensor(log_mel_spec)\n",
    "\n",
    "        return log_mel_spec, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebeac3f-b44e-4973-ac56-14f6b0450be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=False)\n",
    "        model.fc = nn.Linear(512, 512)\n",
    "        model.conv1 = nn.Conv2d(1, 64,\n",
    "                                kernel_size=(7, 7), \n",
    "                                stride=(2, 2), \n",
    "                                padding=(3, 3), \n",
    "                                bias=False)\n",
    "        self.encoder = model\n",
    "        self.cf = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "        self.isdog = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        output1 = self.cf(x)\n",
    "        output2 = self.isdog(x)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ef7ee-57a6-4b79-8f0c-b580a8ae4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from dlcliche.torch_utils import IntraBatchMixup\n",
    "\n",
    "class MyLearner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, learning_rate=3e-4, mixup_alpha=0.4, weight=None, transpose_tfm=True, logger=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.batch_mixer = IntraBatchMixup(nn.CrossEntropyLoss(weight=weight), alpha=mixup_alpha)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.transpose_tfm = transpose_tfm\n",
    "        \n",
    "        self.my_log = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transpose_tfm:\n",
    "            x = x.squeeze(1).transpose(-1, -2) # (B, 1, F, T) -> (B, T, F)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        x, stacked_ys = self.batch_mixer.transform(x, y, train=True)\n",
    "        preds, is_dog = self(x)\n",
    "        loss = self.batch_mixer.criterion(preds, stacked_ys)\n",
    "        \n",
    "        loss2 = nn.BCELoss()(\n",
    "            nn.Sigmoid()(is_dog), (y < 3).type(torch.float).unsqueeze(-1)\n",
    "        )\n",
    "        return loss + 0.3 * loss2\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, split='val'):\n",
    "        x, y = batch\n",
    "        x, stacked_ys = self.batch_mixer.transform(x, y, train=False)\n",
    "        preds, _ = self(x)\n",
    "        loss = self.batch_mixer.criterion(preds, stacked_ys)\n",
    "        \n",
    "        yhat = torch.argmax(preds, dim=1)\n",
    "        acc = accuracy(yhat, y)\n",
    "\n",
    "        self.log(f'{split}_loss', loss, prog_bar=True,logger=True)\n",
    "        self.log(f'{split}_acc', acc, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.my_log.append({\"epoch\": self.trainer.current_epoch,\"val_loss\": loss.item(), \"val_acc\": acc.item()})\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx, split='test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb277d-b3d4-4d0c-93ee-0463af16a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DogDataset(cfg, all_X_train, all_y_train, norm_mean_std=train_mean_std)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, pin_memory=True, num_workers=8)\n",
    "\n",
    "valid_dataset = DogDataset(cfg, all_X_val, all_y_val, norm_mean_std=train_mean_std)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1024, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adacc61-f275-487c-b241-edbe065f2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "model = ResNet()\n",
    "learner = MyLearner(model, learning_rate=5e-4, mixup_alpha=0.2, transpose_tfm=False, weight=torch.Tensor(class_weight).to(device))\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(monitor='val_loss',mode=\"min\", save_top_k=10, every_n_val_epochs=1)\n",
    "trainer = pl.Trainer(gpus=[0], max_epochs=40, callbacks=[checkpoint])\n",
    "trainer.fit(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fad84-47e6-4463-9797-117a5930a833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e2b37-776d-4e3d-9f16-4f72b0cfe960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
